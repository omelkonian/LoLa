==========================
          @K{5m}
==========================

TITLE
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Hi. I am K and this is O and we're going to be talking to you about the three dimensional Dyck language
and our attempts to parse it with a 2-multiple context free grammar. We know this may sound a bit cryptic,
so we're going to start off with some brief definitions and examples to make sure we're all on the same page.

DEFINITIONS
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
First of all, multiple context free grammars are a mildly context sensitive grammar formalism that generalizes
the concept of a standard CFG. They do so by employing the standard CFG operations of concatenation, but now
on tuples of strings instead of just singular strings. More precisely, an m-MCFG is a multiple context free
 grammar that operates on tuples of arity m. Multi-dimensional dyck languages are also a generalization over
the Dyck language, which is the language of well-bracketed parentheses. An N-dimensional Dyck language is defined
over a lexicographically ordered alphabet of N symbols. A word is said to belong in this language if it satifies
two conditions.

	One, all of the N symbols must occur in this word an equal number of times
	and Two, the occurrence of a character in this word must be indirectly preceeded by at least as many occurrences
of all previous characters from the alphabet.

EXAMPLES
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
To make things clearer, we are going to inspect a few examples from the three dimensional case, where our alphabet
is abc. All these words obviously belong to D3, as they satisfy both conditions mentioned.

This is not a D3 word, as it has no occurrences of the letter c.
This is not a D3 word, because the number of 'c's does not match the number of 'a's and 'b's.
And finally this isn't a D3 word either, because even though we have an equal number of 'a's, 'b's and 'c's, the second
'c' appears before the second 'b', breaking condition two.

An easy way to distinguish a D3 word is what we call the first-match policy. What it says is that you may simply pick
the first 'a', create a link from it towards the first 'b', and then connect that one to the first 'c'. Then you can
repeat this process with the second 'abc' triplet and so on and so forth. By the end of the process, every character
in your word must be linked with its matches. This examples already hinds towards the existence of crossing dependencies
in D3, but more on in a bit.

MOTIVATION
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
For now, let us quickly examine what the significance of the problem at hand is.

To begin with, the three dimensional Dyck language is in fact a subset of the so-called MIX language, which we get by simply
removing condition number two. MIX has already been proven expressible by a 2-MCFG, but the amount of shuffling it allows for
 has been characterized linguistically irrelevant. D3 on the other hand is more constrained in nature, thus being closer to
modeling natural language syntax.

Additionally, the uni-dimensional dyck language has found a lot of uses in programming language theory, where it has been used
for the purposes of static analysis. This suffices for most control flows, such as if/then statements. Non-standard flows,
however, such as Python's yield, require a jump to higher dimensions, where all the nice computational properties of CFGs are
lost, which enforces resorting to approximations.

For these reasons, we thought that managing to parse D3 with a 2-MCFG, a computationally efficient alternative to fully context
sensitive approaches, would be a promising endeavour.

With this, O will show you what we tried at first and what sort of problems started arising.

==========================
          @O{5m}
==========================

G0: 3-insertions
- - - - - - - - -
We start out with the most straightforward intuition, namely only inserting ABC-triples. This immediately satisfies the first invariant (#A=#B=#C) and we also insert in order to satisfy the 2nd invariant.

Even this very simple grammar requires 65 rules to cover all cases, where 4 of them are base cases in which we have positioned a single ABC-triple in all possible ways and the rest are inductive cases where we insert another ABC-triple in all possible ways.

STRADDLING
- - - - - -
Now, we quickly find a counter-example where an ABC-triple straddles in-between 2 words, as evidenced by the green BC link in this FMP. So, it turns out that our initial intuition cannot cover such complex interleavings.

META-NOTATION
- - - - - - -
Before we move on to a more complicated grammar, we need to manage the growing complexity, so we come up with a meta-grammatical notation to help us, where - instead of specifying all possible positioning manually, we just declare which partial orders should be satisfied. (These partial orders also implicitly tell us which new terminal symbols we insert)

G1: Meta-G0 + interleaving
- - - - - - - - - - - - - -
So our previous 65-rule grammar reduces to just 2 meta-rules, where the base cases just declare that A must come before B, which comes before C, and our inductive cases naturally respect the previous XY-order and again insert an ABC-triple in-order.

We can now easily extend the previous grammar with the notion interleaving words, where we combine two previous word-nonterminals placing them in any position that respects their internal order XY and ZW.

But we still get straddling counter-examples, although in bigger sizes. What we haven't utilized thus far is the ability to have multiple non-terminals/states.

G2: Adding states
- - - - - - - - - -
We now introduce states that represent incomplete words, either having one additional symbol (A+,B+,C+) or missing one symbol (A-,B-,C-). Our base cases correspond to the positive states, while the combination rules take two states to a single state of the reverse polarity and also model interleaving words, but now we interleave a word-state with any other state and keep the same polariry.

Finally, we have the closure rules to eliminate dual states, namely an A+ coming before an A- and a C- appearing before a C+.

We can see our meta-grammar notation pays off, since we only have 12 meta-rules that eventually expand to approximately 300 concrete rules.

While this greatly reduces the counter-examples, we still find a handful in larger sizes, and even augmenting our grammar with 3-insertions to every state, we still have counter-examples appearing in large words consisting of 5 abc-triples (that is, length=15).

SHALL NOT PARSE
- - - - - - - - -
Well, although we have come up with a complicated grammar of over 600 rules, we still cannot parse the Dyck language completely!!!
Kokos will now show you further attempts to solve this problem.


==========================
          @K{5m}
==========================

REFINING STATES
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
It was at this point when we realized the over-constrained nature of our rules. Even though our incomplete states are expressive
enough with respect to their content, they provide no information whatsoever about the position of the characters they have
extra or are missing. To counter this, each of our incomplete states may be split into a number of refined, position-aware states.
For instance an A-plus may be split into two versions of itself; an A-plus-left, which has the extra 'a' in its left-hand side, and
an A-plus-right which has it in the right-hand side. Similarly, a C-minus may be split into three variations; a C-minus-left, which
has both the extra 'a' and the extra 'b' on its left, a C-minus-right, which has them on its right, and a C-minus-left-right, which
has the extra 'a' in the left and the extra 'b' in the right.

By creating these new states, a vast amount of previously hidden state interactions and tuple element permutations started becoming
apparent. For instance we can now generate some C-minus out of an A-plus-left and some B-plus, where the right element of A-plus, y,
is now placed at the end of the permutation, something that we couldn't have done before.

REFINING STATES: INTERACTIONS
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
These new rules, however, come at a great cost. The number of states we need to consider and all their possible combinations has become
way too large to manage. Manually writing down a full grammar and proofreading through it has become impossible, even at a meta-rule
level. Obviously our meta-grammar approach no longer suffices, and we needed something even more abstract to automate the rule-making
process.

G4: ARIS
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
This is where our last method, Automatic Rule Inference, comes into play. First, each of our new, refined states is matched with a
descriptor of its tuple element's contents. These descriptors essentially tell our system what each state's extra characters are,
but also WHERE they are. Using these descriptors, each state may then be uniquely identified. To create rules out of them, all candidate
permutations between all pairs of states are considered, and then all occurrences of ordered 'abc' triplets are removed. If the
resulting descriptor is the match of a state we have already defined, then this interaction in fact is a valid rule which can be
inferred and used.

ARI: Example
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
As an example, let us consider the case of an A-minus-left-right interacting with a B-minus-left-right. The first has an extra 'b' on its
left and an extra 'c' on its right, while the second has an extra 'a' on its left and an extra 'c' on its right. First, all permutations
that respect the internal order constraints are constructed. Then, from each such permutation we obtain a descriptor. These descriptors
are finally reduced, sometimes in multiple ways; in the first permutation shown, two valid rules arise: one that leads to a C-plus-left,
when the second 'c' gets eliminated, and another that leads to a C-plus-right, when the first 'c' is removed. The second descriptor, on
the other hand, may neither be reduced nor is the mapping of any state, so this permutation does not generate any rule.

This system allows us to remove ourselves almost completely from the rule-making process. We are now able to generate full, extensive grammars
by simply describing the grammar's states on an abstract level, and letting the system work out the rules on its own.

CODE SNIPPET
- - - - - - -
??

RESULTS:
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Despite its appeal, this system was not enough to parse D3 either. As our charts show, we can see that each successive grammar gets significantly
better at parsing, but a small number of counter-examples, or failed parses, continues appearing as words get larger. Additionally, the abstractive
nature of our grammars comes at the cost of a rule size explosion, which is in turn associated with an increase in computation time. As the time
required to check new grammars started becoming prohibitive, we recognized the necessity of switching to more theoretical,
less hacky approaches. O will

==========================
          @O{5m}
==========================

PROMOTION
- - - - - -
It turns out Dyck words have a 1-to-1 correspondence with standard young tableaux, which are rectangular tables (of 3 rows, in our case) which contain the numbers 1-to-N (where N is the length of the word) and, moreover, the elements are strictly increasing both row-wise and column-wise.

On the left, we can see a simple example, where the 1st row indicates the positions of all the As in the string, the 2nd of the Bs, and so forth.

Additionally, if we read it column-by-column, we get the FMP which was the most intuitive all along.

Now, there is an interesting operation you can apply to these tableaux, namely "le jeu de taquin" (or just promotion), where
you replace the number 1 with a dot and decrement all other elements by 1. You then move the dot either right or down, depending on where the smallest element is. When you eventually reach the lower right part, you replace the dot with the number N.

You end up in another standard young tableau, which corresponds to another Dyck word! This operation is in fact cyclic, meaning
you will eventually circle back to your original tableau, in at most N rounds. Therefore, it separates the Dyck word space in disjoint equivalence classes, allowing us to somehow divide the problem into smaller ones.

WEBS
- - -
Another interesting correspondence exist with a combinatorial object, called spider webs (which are a specific kind of planar graphs). The mathematicians who came up with that, call Dyck words Yamanouchi words which have 10\bar{1} instead of ABC, but it is essentially the same problem.

We have no time to go into the definition of spider webs, but the act of promotion on Young Tableaux corresponds to rotation of these graphs and, most importantly, there exists an algorithm which transforms a given Dyck word to the corresponding spider web using the so-called "growth rules".

Now, if we replace 1 with A, 0 with B, \bar{1} with C and assign positive polarity to nodes which have outgoing edges and negative polarity otherwise, we come up with these growth rules, which are very similar to the multiple-state rules you saw earlier.

When we first realised that, our minds were blown since if we modelled all these growth rules with our meta-rules, we would have a complete Dyck grammar!!!

But, unfortunately, we quickly realised we cannot model the 4-node rules which have two output states, since we can only have one output...

VIZ
- - -
After all, we did not manage to utilize these mathematical correspondences to solve our problem, but we nonetheless developed another python package (called dyckviz), which allows you to experiment with different equivalence classes and check the corresponding combinatorial objects.

You essentially give a Dyck word and get the complete matching equivalence class, coloured in the terminal using the FMP.
At the same time you get an automatically-generated pdf containing all the spider webs corresponding to the words in this equivalence class.

CONCLUSION
- - - - - -
To conclude, we did not manage to find an 2-MCFG for the Dyck language of dimension 3, but had lots of fun and twists along the way.

We are now quite pessimistic on whether there exists a 2-MCFG, but we cannot yet exclude the possibility formally.
Nonetheless, we are confident we've found a complete 3-MCFG and we are currently trying to mechanize a proof in the COQ programming  language.

We chose to do it that way, since we have to consider a lot of cases in our proof as our meta-grammar consists of 2 meta-rules which expand to ~100 rules...

QUESTIONS
- - - - -
Thank you for your attention and feel free to ask any questions
